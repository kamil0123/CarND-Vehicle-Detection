{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Self-Driving Car Engineer Nanodegree Program\n",
    "## Vehicle Detection Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "- Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a Linear SVM classifier:\n",
    "    - Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector.\n",
    "    - Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "- Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "- Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "- Estimate a bounding box for vehicles detected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Writeup / README\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. Here is a template writeup for this project you can use as a guide and a starting point.\n",
    "You're reading it!\n",
    "\n",
    "Whole project code is in the Jupyter notebook CarND-Vehicle-Detection-Project.ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Oriented Gradients (HOG)\n",
    "#### 1. Explain how (and identify where in your code) you extracted HOG features from the training images.\n",
    "First I load all iamge names. I hold them in two seperate lists for vehicle and non vehicle images, and extract HOG features for both of them. After getting HOG features I Split up data into randomized training and test sets.\n",
    "\n",
    "The code for extracting HOG is contained in the functions get_hog_features(), which returns HOG features of one image, and extract_features(), which returns list of HOG features of all images. To calculate HOG I'm using function hog from skimage.feature of scikit collection.\n",
    "![title](./output_images/vehicleHog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "Final choice of HOG parameters was depended of SVC accuracy and time needed to get it. \n",
    "\n",
    "To set up paramters values I decided to use 8 pixels per cell and 2 cells per block, and then try different color spaces (RGB, HSV, YUV), number of orientetions and number of color channels. I tried various combinations of parameters. I couldn't get at one time good accuracy with fast training time so finally I just focused on accuracy.\n",
    "Finnaly I got over 98% accuracy with YUV colorspace, all 3 channels and 11 orientations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n",
    "I resign from color color features and trained a linear SVM usign olny HOG features. \n",
    "The code of this is in part called \"Train a classifier\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window Search\n",
    "\n",
    "#### 1. Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is contained in \"Sliding Window Search\" part of ipynb file.\n",
    "The main function is find_cars(), which was showed during the course and extract features using hog sub-sampling and make predictions.\n",
    "The find_cars() only has to extract hog features once and then can be sub-sampled to get all of its overlaying windows. Each window is defined by a scaling factor where a scale of 1 would result in a window that's 8 x 8, and  and 8 pix per cell, cells then the overlap of each window is in terms of the cell distance. A cells_per_step = 2 would result in a search window overlap of 75%. \n",
    "I'm running this function mul multiple times for different scale values to generate multiple-scaled search windows.\n",
    "I start with scale 1.0 near middle of image (400 pxs from top of image), go to the buttom. Change scales to bigger and finish with 3.5 on the bottom of image. \n",
    "I decided which scale value use in which pixel range by comparing slide windows size with car sizes on test images.\n",
    "![title](./output_images/findingBboxes2.png)\n",
    "![title](./output_images/findingBboxes6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working. What did you do to optimize the performance of your classifier?\n",
    "\n",
    "After sliding windows search I got some false positive detections.\n",
    "To remove them I created of heat map of all detections.\n",
    "![title](./output_images/heatMap.png)\n",
    "Then reject areas affected by false positives by thresholding. I'm using hardcoded valuse of threshold in test image pipeline , but in video pipeline it changes with the number of frames from which I'm getting boxes boundries.\n",
    "Finally I'm using function label() from scipy.ndimage.measurements to get number of found cars and I'm drawing rectenge boundries on image function with draw_labeled_bboxes().\n",
    "![title](./output_images/Labels.png)\n",
    "![title](./output_images/afterLabels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Video Implementation\n",
    "#### 1. Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.) \n",
    "Here's a link to my [video result.](project_video_out.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "The code for this part is contained in \"Sliding Window Search\" section. \n",
    "Video pipeline works analogous to the one described earlier image pipeline. \n",
    "The only difference is that I created Boxes class in which I hold boundary boxes of last 16 frames and use them to create heat map and calculate threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?\n",
    "\n",
    "Main problem was to find parameters of HOG, which gives good accuracy. I finnaly get over 98%, but with the cost of quite long time of calculating. \n",
    "Other problem was to remove false positives from video. They appear on some video frames. Good solution to remove them was creating of class, which allows find cars and calculate their boundries not on a single frame, but on last n-frames (I used 16).\n",
    "\n",
    "I still have few positive negatives so I think video with more color, shadow changes would be a challenge for my algorithm. \n",
    "\n",
    "I used only HOG features. To make project more robust I think adding some color extracting features methods, like Spatial Binning of Color or Histograms of Color would be a good idea."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
